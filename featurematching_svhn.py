# -*- coding: utf-8 -*-
"""featurematching_svhn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DB00Sruf2RiXpZpiGC2FhLAJrqNSRf9E
"""

!pip install matplotlib

!pip install nn

!pip install torch

import sys
sys.path.append('/content/drive/MyDrive/Colab Notebooks/project/improved-gan-master/mnist_svhn_cifar10')

import argparse
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
from torch.nn.utils import weight_norm
#import svhn_data

# Function to show generated images from the generator model
def show_generated_images(generator, num_images=16):
    # Generate random noise as input
    noise = torch.randn(num_images, 100).cuda()

    # Generate images with the generator model
    with torch.no_grad():
        generated_data = generator(noise).cpu()

    # Reshape to 32x32 for visualization and scale from [0, 1]
    generated_data = generated_data.view(num_images, 3, 32, 32).permute(0, 2, 3, 1).numpy()

    # Set up a grid for displaying images
    fig, axes = plt.subplots(4, 4, figsize=(8, 8))
    for i, ax in enumerate(axes.flatten()):
        ax.imshow(generated_data[i])
        ax.axis("off")
    plt.tight_layout()
    plt.show()

# Define parameters directly (instead of using argparse)
args = argparse.Namespace(seed=1, seed_data=1, unlabeled_weight=1.0, batch_size=100, count=10)

# Display arguments to confirm they're set
print(args)

# fixed random seeds
torch.manual_seed(args.seed)
np.random.seed(args.seed_data)

from torchvision.datasets import SVHN
from torchvision import transforms

# Define transformations for the SVHN dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,)),
])

# Load the SVHN dataset (download if necessary)
train_data = SVHN(root='./data', split='train', transform=transform, download=True)
test_data = SVHN(root='./data', split='test', transform=transform, download=True)
trainx = train_data.data
trainy = train_data.labels

# Define labeled data (you can modify this part based on your requirements)
num_labeled = 500  # Number of labeled examples to use
labeled_idx = np.random.choice(len(trainx), num_labeled, replace=False)

trainx_labeled = trainx[labeled_idx]
trainy_labeled = trainy[labeled_idx]

# DataLoader for labeled data
labeled_dataset = TensorDataset(torch.tensor(trainx_labeled).float(), torch.tensor(trainy_labeled).long())
labeled_loader = DataLoader(labeled_dataset, batch_size=100, shuffle=True)

# DataLoader for unlabeled data
unlabeled_idx = np.setdiff1d(np.arange(len(trainx)), labeled_idx)
trainx_unlabeled = trainx[unlabeled_idx]

unlabeled_dataset = TensorDataset(torch.tensor(trainx_unlabeled).float())
unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=100, shuffle=True)

# DataLoader for testing data
test_dataset = TensorDataset(torch.tensor(test_data.data).float(), torch.tensor(test_data.labels).long())
test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)

# Generator model
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(100, 500),
            nn.ReLU(),
            nn.Linear(500, 500),
            nn.ReLU(),
            nn.Linear(500, 3 * 32 * 32),
            nn.Tanh()
        )

    def forward(self, x):
        return self.model(x)

# Discriminator model
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(3 * 32 * 32, 1000),
            nn.ReLU(),
            nn.Linear(1000, 500),
            nn.ReLU(),
            nn.Linear(500, 250),
            nn.ReLU(),
            nn.Linear(250, 250),
            nn.ReLU(),
            nn.Linear(250, 10)
        )

    def forward(self, x):
        return self.model(x)

gen = Generator()
disc = Discriminator()
gen_optimizer = optim.Adam(gen.parameters(), lr=0.003)
disc_optimizer = optim.Adam(disc.parameters(), lr=0.003)
criterion = nn.CrossEntropyLoss()

# Training loop
for epoch in range(300):
    start_time = time.time()
    loss_lab = 0.0
    loss_unl = 0.0
    train_err = 0.0

    # Train Discriminator on labeled and unlabeled data
    for (x_lab, labels) in labeled_loader:
        x_lab, labels = x_lab.view(-1, 3 * 32 * 32), labels

        # Labeled data loss
        disc_optimizer.zero_grad()
        output_lab = disc(x_lab)
        labeled_loss = criterion(output_lab, labels)

        # Unlabeled data loss
        noise = torch.randn(args.batch_size, 100)
        gen_data = gen(noise).detach()
        output_unl = disc(x_lab)
        output_fake = disc(gen_data)

        unlabeled_loss = -0.5 * torch.mean(torch.logsumexp(output_unl, dim=1)) + 0.5 * torch.mean(torch.nn.functional.softplus(torch.logsumexp(output_unl, dim=1))) + 0.5 * torch.mean(torch.nn.functional.softplus(torch.logsumexp(output_fake, dim=1)))

        disc_loss = labeled_loss + args.unlabeled_weight * unlabeled_loss
        disc_loss.backward()
        disc_optimizer.step()

        loss_lab += labeled_loss.item()
        loss_unl += unlabeled_loss.item()
        train_err += (output_lab.argmax(dim=1) != labels).float().mean().item()

    # Train Generator
    for (x_unl, _) in labeled_loader:
        x_unl = x_unl.view(-1, 3 * 32 * 32)
        gen_optimizer.zero_grad()

        noise = torch.randn(args.batch_size, 100)
        gen_data = gen(noise)

        mom_gen = disc(gen_data).mean(dim=0)
        mom_real = disc(x_unl).mean(dim=0)
        gen_loss = torch.mean((mom_gen - mom_real) ** 2)

        gen_loss.backward()
        gen_optimizer.step()

    loss_lab /= len(labeled_loader)
    loss_unl /= len(labeled_loader)
    train_err /= len(labeled_loader)

    # Testing phase
    test_err = 0.0
    with torch.no_grad():
        for x_lab, labels in test_loader:
            x_lab, labels = x_lab.view(-1, 3 * 32 * 32), labels
            output_test = disc(x_lab)
            test_err += (output_test.argmax(dim=1) != labels).float().mean().item()
    test_err /= len(test_loader)

    print(f"Epoch {epoch+1}, Time: {time.time() - start_time:.2f}s, Labeled Loss: {loss_lab:.4f}, Unlabeled Loss: {loss_unl:.4f}, Train Error: {train_err:.4f}, Test Error: {test_err:.4f}")
    if epoch == 299:  # Last epoch
        show_generated_images(gen)

"""# specify generative mo# specify discriminative model

# Define the Discriminator model using PyTorch
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        # First block of Conv2D layers
        self.conv1 = weight_norm(nn.Conv2d(3, 64, kernel_size=3, padding=1))
        self.conv2 = weight_norm(nn.Conv2d(64, 64, kernel_size=3, padding=1))
        self.conv3 = weight_norm(nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=2))
        
        # Second block of Conv2D layers
        self.conv4 = weight_norm(nn.Conv2d(64, 128, kernel_size=3, padding=1))
        self.conv5 = weight_norm(nn.Conv2d(128, 128, kernel_size=3, padding=1))
        self.conv6 = weight_norm(nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=2))

        # Network-in-Network (NIN) layers
        self.nin1 = weight_norm(nn.Conv2d(128, 128, kernel_size=1))
        self.nin2 = weight_norm(nn.Conv2d(128, 128, kernel_size=1))

        # Global pooling layer (Global Average Pooling)
        self.global_pool = nn.AdaptiveAvgPool2d(1)

        # Fully connected layer (output to 10 classes)
        self.fc = weight_norm(nn.Linear(128, 10))

        # Dropout layers
        self.dropout1 = nn.Dropout(0.2)
        self.dropout2 = nn.Dropout(0.5)
        self.dropout3 = nn.Dropout(0.5)

    def forward(self, x):
        x = self.dropout1(F.leaky_relu(self.conv1(x), negative_slope=0.2))
        x = F.leaky_relu(self.conv2(x), negative_slope=0.2)
        x = F.leaky_relu(self.conv3(x), negative_slope=0.2)
        
        x = self.dropout2(F.leaky_relu(self.conv4(x), negative_slope=0.2))
        x = F.leaky_relu(self.conv5(x), negative_slope=0.2)
        x = F.leaky_relu(self.conv6(x), negative_slope=0.2)
        
        x = F.leaky_relu(self.nin1(x), negative_slope=0.2)
        x = F.leaky_relu(self.nin2(x), negative_slope=0.2)

        # Global average pooling
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layer

        # Output layer
        x = self.fc(x)
        return x

# Initialize the Discriminator model
disc_model = Discriminator()

# Print model summary
print(disc_model)

# Example input tensor with batch_size=64 and image size 32x32 (e.g., SVHN dataset)
input_tensor = torch.randn(64, 3, 32, 32)
output = disc_model(input_tensor)
print(output.shape)  # Should output (64, 10) for a batch of 64 samples and 10 classes


"""